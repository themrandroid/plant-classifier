{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67c3584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import glob\n",
    "import random\n",
    "from PIL import Image\n",
    "from tqdm import tqdm \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # type: ignore\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image # type: ignore\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54671248",
   "metadata": {},
   "source": [
    "### Generate Taxon IDs for the plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ce0d5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Species Taxon IDs file exists\n"
     ]
    }
   ],
   "source": [
    "output_file = \"species_taxon_ids.csv\"\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    print(\"Species Taxon IDs file exists\")\n",
    "else:\n",
    "    # 1. Define your species list\n",
    "    species = \"species_list.csv\"\n",
    "    species_list = []\n",
    "\n",
    "    with open(species, mode='r', newline='') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            species_list.append(tuple(row))\n",
    "\n",
    "    # 2. Function to get taxon ID\n",
    "    def get_taxon_id(name):\n",
    "        url = f\"https://api.inaturalist.org/v1/search?q={name}&sources=taxa\"\n",
    "        # params = {\"q\": scientific_name, \"rank\": \"species\"}\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "\n",
    "        if data.get(\"results\"):\n",
    "            taxon = data[\"results\"][0][\"record\"]\n",
    "            return taxon[\"id\"], taxon.get(\"preferred_common_name\", \"\")\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    # 3. Fetch IDs & save to CSV\n",
    "    with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"common_name\", \"scientific_name\", \"taxon_id\", \"inat_common_name\"])\n",
    "\n",
    "        for sci_name, common_name in species_list:\n",
    "            taxon_id, inat_common = get_taxon_id(sci_name)\n",
    "            writer.writerow([sci_name, common_name, taxon_id, inat_common])\n",
    "            print(f\"{sci_name} → Taxon ID: {taxon_id}, iNat Common: {inat_common}\")\n",
    "            time.sleep(1)  # Avoid rate limiting\n",
    "\n",
    "    print(f\"Taxon IDs saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c253a964",
   "metadata": {},
   "source": [
    "### Download the images using the Generated Taxon IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b1c7925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Rosa_indica (already has enough images)\n",
      "Skipping Helianthus_annuus (already has enough images)\n",
      "Skipping Hibiscus_rosa-sinensis (already has enough images)\n",
      "Skipping Tagetes_erecta (already has enough images)\n",
      "Skipping Jasminum_sambac (already has enough images)\n",
      "Skipping Lavandula_angustifolia (already has enough images)\n",
      "Skipping Orchidaceae_spp. (already has enough images)\n",
      "Skipping Lilium_spp. (already has enough images)\n",
      "Skipping Tulipa_spp. (already has enough images)\n",
      "Skipping Bellis_perennis (already has enough images)\n",
      "Skipping Bougainvillea_glabra (already has enough images)\n",
      "Skipping Chrysanthemum_indicum (already has enough images)\n",
      "Skipping Nelumbo_nucifera (already has enough images)\n",
      "Skipping Pelargonium_spp. (already has enough images)\n",
      "Skipping Narcissus_spp. (already has enough images)\n",
      "Skipping Petunia_spp. (already has enough images)\n",
      "Skipping Zinnia_elegans (already has enough images)\n",
      "Skipping Dianthus_caryophyllus (already has enough images)\n",
      "Skipping Ipomoea_purpurea (already has enough images)\n",
      "Skipping Antirrhinum_majus (already has enough images)\n",
      "Skipping Papaver_rhoeas (already has enough images)\n",
      "Skipping Gladiolus_spp. (already has enough images)\n",
      "Skipping Camellia_japonica (already has enough images)\n",
      "Skipping Dahlia_pinnata (already has enough images)\n",
      "Skipping Cosmos_bipinnatus (already has enough images)\n",
      "Skipping Mangifera_indica (already has enough images)\n",
      "Skipping Musa_spp. (already has enough images)\n",
      "Skipping Citrus_sinensis (already has enough images)\n",
      "Skipping Malus_domestica (already has enough images)\n",
      "Skipping Psidium_guajava (already has enough images)\n",
      "Skipping Carica_papaya (already has enough images)\n",
      "Skipping Ananas_comosus (already has enough images)\n",
      "Skipping Citrullus_lanatus (already has enough images)\n",
      "Skipping Vitis_vinifera (already has enough images)\n",
      "Downloaded 100 images for Citrus_limon\n",
      "Skipping Cocos_nucifera (already has enough images)\n",
      "Skipping Persea_americana (already has enough images)\n",
      "Skipping Fragaria_×_ananassa (already has enough images)\n",
      "Skipping Pyrus_communis (already has enough images)\n",
      "Skipping Prunus_avium (already has enough images)\n",
      "Skipping Prunus_domestica (already has enough images)\n",
      "Skipping Ficus_carica (already has enough images)\n",
      "Skipping Olea_europaea (already has enough images)\n",
      "Skipping Actinidia_deliciosa (already has enough images)\n",
      "Skipping Punica_granatum (already has enough images)\n",
      "Skipping Passiflora_edulis (already has enough images)\n",
      "Skipping Artocarpus_heterophyllus (already has enough images)\n",
      "Skipping Hylocereus_undatus (already has enough images)\n",
      "Skipping Phoenix_dactylifera (already has enough images)\n",
      "Downloaded 100 images for Litchi_chinensis\n",
      "Skipping Azadirachta_indica (already has enough images)\n",
      "Skipping Acacia_spp. (already has enough images)\n",
      "Skipping Eucalyptus_globulus (already has enough images)\n",
      "Skipping Adansonia_digitata (already has enough images)\n",
      "Skipping Acer_spp. (already has enough images)\n",
      "Skipping Quercus_spp. (already has enough images)\n",
      "Skipping Pinus_spp. (already has enough images)\n",
      "Skipping Mangifera_indica (already has enough images)\n",
      "Skipping Tamarindus_indica (already has enough images)\n",
      "Skipping Psidium_guajava (already has enough images)\n",
      "Skipping Arecaceae_spp. (already has enough images)\n",
      "Skipping Hevea_brasiliensis (already has enough images)\n",
      "Skipping Tectona_grandis (already has enough images)\n",
      "Skipping Swietenia_macrophylla (already has enough images)\n",
      "Skipping Ficus_benghalensis (already has enough images)\n",
      "Skipping Ceiba_pentandra (already has enough images)\n",
      "Skipping Cassia_fistula (already has enough images)\n",
      "Skipping Delonix_regia (already has enough images)\n",
      "Skipping Jacaranda_mimosifolia (already has enough images)\n",
      "Skipping Anacardium_occidentale (already has enough images)\n",
      "Skipping Theobroma_cacao (already has enough images)\n",
      "Skipping Artocarpus_altilis (already has enough images)\n",
      "Skipping Morus_alba (already has enough images)\n",
      "Skipping Solanum_betaceum (already has enough images)\n",
      "Skipping Moringa_oleifera (already has enough images)\n",
      "Skipping Taraxacum_officinale (already has enough images)\n",
      "Skipping Trifolium_spp. (already has enough images)\n",
      "Skipping Plantago_major (already has enough images)\n",
      "Skipping Lantana_camara (already has enough images)\n",
      "Skipping Aloe_barbadensis_miller (already has enough images)\n",
      "Skipping Ocimum_basilicum (already has enough images)\n",
      "Skipping Mentha_spp. (already has enough images)\n",
      "Downloaded 100 images for Cymbopogon_citratus\n",
      "Skipping Thymus_vulgaris (already has enough images)\n",
      "Skipping Salvia_officinalis (already has enough images)\n",
      "Skipping Origanum_vulgare (already has enough images)\n",
      "Skipping Salvia_rosmarinus (already has enough images)\n",
      "Skipping Zingiber_officinale (already has enough images)\n",
      "Downloaded 100 images for Curcuma_longa\n",
      "Skipping Coriandrum_sativum (already has enough images)\n",
      "Downloaded 100 images for Trigonella_foenum-graecum\n",
      "Skipping Ocimum_tenuiflorum (already has enough images)\n",
      "Skipping Matricaria_chamomilla (already has enough images)\n",
      "Skipping Echinacea_purpurea (already has enough images)\n",
      "Skipping Mentha_×_piperita (already has enough images)\n",
      "Skipping Calendula_officinalis (already has enough images)\n",
      "Skipping Lavandula_angustifolia (already has enough images)\n",
      "Skipping Achillea_millefolium (already has enough images)\n",
      "Skipping Urtica_dioica (already has enough images)\n",
      "Skipping Artemisia_absinthium (already has enough images)\n"
     ]
    }
   ],
   "source": [
    "TAXON_IDS = \"species_taxon_ids.csv\"\n",
    "OUTPUT_DIR = \"data\"\n",
    "IMAGES_PER_SPECIES = 100\n",
    "data_folder = \"data\"\n",
    "\n",
    "if os.path.exists(data_folder and os.path.isdir(data_folder)):\n",
    "    subfolders = [x for x in os.listdir(data_folder) if os.path.isdir(os.path.join(data_folder, x))]\n",
    "    if len(subfolders) == 100:\n",
    "        print(\"All images downloaded\")\n",
    "    else:\n",
    "        def download_images_for_taxon(taxon_id, folder_name, limit=50):\n",
    "            os.makedirs(folder_name, exist_ok=True)\n",
    "            page = 1\n",
    "            downloaded = set(os.listdir(folder_name))\n",
    "            counter = len(downloaded)\n",
    "\n",
    "            while len(downloaded) < limit:\n",
    "                url = \"https://api.inaturalist.org/v1/observations\"\n",
    "                params = {\n",
    "                    \"taxon_id\": taxon_id,\n",
    "                    \"quality_grade\": \"research\",\n",
    "                    \"per_page\": 200,\n",
    "                    \"page\": page,\n",
    "                    \"order_by\": \"votes\",\n",
    "                    \"order\": \"desc\",\n",
    "                    # Public Licenced images only\n",
    "                    \"license\": \"cc0,cc-by,cc-by-sa\",\n",
    "                    \"photo_license\": \"cc0,cc-by,cc-by-sa\"\n",
    "                }\n",
    "\n",
    "                response = requests.get(url, params=params)\n",
    "                data = response.json()\n",
    "\n",
    "                if not data.get(\"results\"):\n",
    "                    break\n",
    "\n",
    "                for obs in data[\"results\"]:\n",
    "                    for photo in obs.get(\"photos\", []):\n",
    "                        img_url = photo[\"url\"].replace(\"square\", \"medium\")\n",
    "                        if img_url in downloaded:\n",
    "                            continue\n",
    "                        downloaded.add(img_url)\n",
    "\n",
    "                        # Use unique filename: photo_id + counter\n",
    "                        photo_id = photo[\"id\"]\n",
    "                        img_ext = os.path.splitext(urlparse(img_url).path)[1]\n",
    "                        img_name = f\"{photo_id}_{counter}{img_ext}\"\n",
    "                        img_path = os.path.join(folder_name, img_name)\n",
    "\n",
    "                        try:\n",
    "                            img_data = requests.get(img_url).content\n",
    "                            with open(img_path, \"wb\") as f:\n",
    "                                f.write(img_data)\n",
    "                            counter += 1\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error saving {img_url}: {e}\")\n",
    "\n",
    "                        if counter >= limit:\n",
    "                            return\n",
    "                page += 1\n",
    "                time.sleep(0.5)  # avoid API rate limit\n",
    "\n",
    "        # Main loop\n",
    "        with open(TAXON_IDS, newline=\"\", encoding=\"utf-8\") as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                sci_name = row[\"scientific_name\"].replace(\" \", \"_\")\n",
    "                taxon_id = row[\"taxon_id\"]\n",
    "\n",
    "                if not taxon_id or taxon_id.lower() == \"none\":\n",
    "                    print(f\"Skipping {sci_name} (no taxon_id)\")\n",
    "                    continue\n",
    "\n",
    "                folder_path = os.path.join(OUTPUT_DIR, sci_name)\n",
    "                if os.path.isdir(folder_path) and len(os.listdir(folder_path)) >= IMAGES_PER_SPECIES:\n",
    "                    print(f\"Skipping {sci_name} (already has enough images)\")\n",
    "                    continue\n",
    "                download_images_for_taxon(taxon_id, folder_path, IMAGES_PER_SPECIES)\n",
    "                print(f\"Downloaded {IMAGES_PER_SPECIES} images for {sci_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646856a3",
   "metadata": {},
   "source": [
    "### Scrape the Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d1c86f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plant Data JSON file exists\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SETTINGS\n",
    "CSV_FILE = \"species_taxon_ids.csv\"\n",
    "OUTPUT_JSON = \"plant_data.json\"\n",
    "DESCRIPTION_SENTENCES = 2\n",
    "SLEEP_TIME = 0.5\n",
    "output_file = \"plant_data.json\"\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    print(\"Plant Data JSON file exists\")\n",
    "else:\n",
    "    # Wikipedia API helper\n",
    "    def get_wikipedia_page(query):\n",
    "        url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{query.replace(' ', '_')}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        return None\n",
    "\n",
    "    def get_full_wikipedia_text(query):\n",
    "        url = f\"https://en.wikipedia.org/w/api.php?action=query&prop=extracts&explaintext&format=json&titles={query.replace(' ', '_')}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "            if \"extract\" in page:\n",
    "                return page[\"extract\"]\n",
    "        return None\n",
    "\n",
    "    # Main script\n",
    "    plant_info = {}\n",
    "\n",
    "    with open(CSV_FILE, newline=\"\", encoding=\"utf-8\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            sci_name = row[\"scientific_name\"].strip()\n",
    "            common_name = row[\"common_name\"].strip()\n",
    "            inat_common_name = row[\"inat_common_name\"].strip()\n",
    "\n",
    "            print(f\"Processing {common_name} ({sci_name})...\")\n",
    "            # Get description\n",
    "            data = get_wikipedia_page(sci_name) or get_wikipedia_page(common_name)\n",
    "            description = None\n",
    "            if data and \"extract\" in data:\n",
    "                desc_text = data[\"extract\"]\n",
    "                description = '.'.join(desc_text.split('.')[:DESCRIPTION_SENTENCES]) + '.'\n",
    "\n",
    "            if not description:\n",
    "                description = \"Description not available.\"\n",
    "            # Get fun fact\n",
    "            # --------------------\n",
    "            full_text = get_full_wikipedia_text(sci_name) or get_full_wikipedia_text(common_name)\n",
    "            fun_fact = None\n",
    "            if full_text:\n",
    "                paragraphs = [p.strip() for p in full_text.split(\"\\n\") if len(p.strip()) > 50]\n",
    "                if len(paragraphs) > 1:\n",
    "                    # Take second paragraph if it seems interesting\n",
    "                    fact_candidate = paragraphs[1]\n",
    "                    fact_candidate = re.sub(r'\\[\\d+\\]', '', fact_candidate)  # Remove citations\n",
    "                    fun_fact = '.'.join(fact_candidate.split('.')[:2]).strip() + '.'\n",
    "\n",
    "            if not fun_fact:\n",
    "                fun_fact = \"Fun-fact not available\"\n",
    "            # Store data\n",
    "            plant_info[sci_name] = {\n",
    "                \"common_name\": common_name,\n",
    "                \"inat_common_name\": inat_common_name,\n",
    "                \"description\": description,\n",
    "                \"fun_fact\": fun_fact\n",
    "            }\n",
    "\n",
    "            print(f\"Added {common_name}\")\n",
    "            time.sleep(SLEEP_TIME)\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(plant_info, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"\\nAll done! Data saved to {OUTPUT_JSON}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac65dbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citrus_limon: 48 images\n",
      "Litchi_chinensis: 72 images\n",
      "94\n"
     ]
    }
   ],
   "source": [
    "# Path to your main data folder\n",
    "data_dir = \"data\"\n",
    "\n",
    "# Allowed image extensions (case-insensitive)\n",
    "image_exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".tiff\"}\n",
    "\n",
    "i = 0\n",
    "# Walk through each subfolder\n",
    "for subfolder in sorted(os.listdir(data_dir)):\n",
    "    subfolder_path = os.path.join(data_dir, subfolder)\n",
    "\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Count files with valid extensions\n",
    "        count = sum(\n",
    "            1 for f in os.listdir(subfolder_path)\n",
    "            if os.path.splitext(f)[1].lower() in image_exts\n",
    "        )\n",
    "        if count < 100:\n",
    "            print(f\"{subfolder}: {count} images\")\n",
    "        elif count == 100:\n",
    "            i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8d2118c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\Citrus_limon now has 100 images\n",
      "data\\Litchi_chinensis now has 100 images\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2]\n",
    ")\n",
    "\n",
    "folders = [r\"data\\Citrus_limon\", r\"data\\Litchi_chinensis\"]   # subfolder with only 48 or 72 images\n",
    "target_count = 100\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(folder)\n",
    "    current_count = len(files)\n",
    "    i = 0\n",
    "    while current_count < target_count:\n",
    "        img_path = os.path.join(folder, files[i % len(files)])\n",
    "        img = image.load_img(img_path, target_size=(224, 224))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "\n",
    "        for batch in datagen.flow(x, batch_size=1, save_to_dir=folder, save_prefix=\"aug\", save_format=\"jpg\"):\n",
    "            current_count += 1\n",
    "            break  # one new image per loop\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    print(f\"{folder} now has {current_count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c50e0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "# Path to your main data folder\n",
    "data_dir = \"data\"\n",
    "\n",
    "# Allowed image extensions (case-insensitive)\n",
    "image_exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".tiff\"}\n",
    "\n",
    "i = 0\n",
    "# Walk through each subfolder\n",
    "for subfolder in sorted(os.listdir(data_dir)):\n",
    "    subfolder_path = os.path.join(data_dir, subfolder)\n",
    "\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Count files with valid extensions\n",
    "        count = sum(\n",
    "            1 for f in os.listdir(subfolder_path)\n",
    "            if os.path.splitext(f)[1].lower() in image_exts\n",
    "        )\n",
    "        if count < 100:\n",
    "            print(f\"{subfolder}: {count} images\")\n",
    "        elif count == 100:\n",
    "            i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "688ae5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed: data\\Fragaria_×_ananassa → data\\Fragaria_x_ananassa\n",
      "Renamed: data\\Mentha_×_piperita → data\\Mentha_x_piperita\n",
      "\n",
      "Folder renaming complete.\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = r\"data\"  # your root dataset folder\n",
    "REPLACEMENTS = {\n",
    "    \"×\": \"x\",  # replace multiplication sign with lowercase x\n",
    "}\n",
    "\n",
    "def safe_name(name):\n",
    "    new_name = name\n",
    "    for bad_char, replacement in REPLACEMENTS.items():\n",
    "        new_name = new_name.replace(bad_char, replacement)\n",
    "    return new_name\n",
    "\n",
    "def rename_folders(root_dir):\n",
    "    for current_dir, dirs, files in os.walk(root_dir, topdown=False):\n",
    "        for d in dirs:\n",
    "            new_d = safe_name(d)\n",
    "            if new_d != d:\n",
    "                old_path = os.path.join(current_dir, d)\n",
    "                new_path = os.path.join(current_dir, new_d)\n",
    "                os.rename(old_path, new_path)\n",
    "                print(f\"Renamed: {old_path} → {new_path}\")\n",
    "\n",
    "rename_folders(DATASET_DIR)\n",
    "print(\"\\nFolder renaming complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcda31a",
   "metadata": {},
   "source": [
    "### Preprocess the Images for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dcc078e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 96 species folders.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing species: 100%|██████████| 96/96 [01:53<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing complete!\n",
      "Processed dataset saved in: processed_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# SETTINGS\n",
    "RAW_DATA_DIR = \"data\"\n",
    "OUTPUT_DIR = \"processed_data\"\n",
    "IMAGE_SIZE = (224, 224)\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.1  # Remaining 0.1 will be test\n",
    "ALLOWED_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\"}\n",
    "\n",
    "# Helper: check valid image\n",
    "def is_valid_image(path):\n",
    "    try:\n",
    "        with Image.open(path) as img:\n",
    "            img.verify()\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Step 1: Clean and gather all images\n",
    "all_species = os.listdir(RAW_DATA_DIR)\n",
    "print(f\"Found {len(all_species)} species folders.\")\n",
    "\n",
    "for species in tqdm(all_species, desc=\"Processing species\"):\n",
    "    species_path = os.path.join(RAW_DATA_DIR, species)\n",
    "    if not os.path.isdir(species_path):\n",
    "        continue\n",
    "\n",
    "    images = []\n",
    "    for ext in ALLOWED_EXTENSIONS:\n",
    "        images.extend(glob.glob(os.path.join(species_path, f\"*{ext}\")))\n",
    "\n",
    "    # Remove invalid files\n",
    "    valid_images = [img for img in images if is_valid_image(img)]\n",
    "\n",
    "    # Shuffle for randomness\n",
    "    random.shuffle(valid_images)\n",
    "\n",
    "    # Step 2: Train/Val/Test split\n",
    "    n_total = len(valid_images)\n",
    "    n_train = int(n_total * TRAIN_SPLIT)\n",
    "    n_val = int(n_total * VAL_SPLIT)\n",
    "    n_test = n_total - n_train - n_val\n",
    "\n",
    "    splits = {\n",
    "        \"train\": valid_images[:n_train],\n",
    "        \"val\": valid_images[n_train:n_train + n_val],\n",
    "        \"test\": valid_images[n_train + n_val:]\n",
    "    }\n",
    "\n",
    "    # Step 3: Save resized images to OUTPUT_DIR\n",
    "    for split_name, file_list in splits.items():\n",
    "        split_dir = os.path.join(OUTPUT_DIR, split_name, species)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "        for file_path in file_list:\n",
    "            try:\n",
    "                with Image.open(file_path) as img:\n",
    "                    img = img.convert(\"RGB\")  # Ensure 3 channels\n",
    "                    img = img.resize(IMAGE_SIZE)\n",
    "                    file_name = os.path.basename(file_path)\n",
    "                    img.save(os.path.join(split_dir, file_name), \"JPEG\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "print(\"\\nPreprocessing complete!\")\n",
    "print(f\"Processed dataset saved in: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fade221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed: processed_data\\test\\Fragaria_×_ananassa → processed_data\\test\\Fragaria_x_ananassa\n",
      "Renamed: processed_data\\test\\Mentha_×_piperita → processed_data\\test\\Mentha_x_piperita\n",
      "Renamed: processed_data\\train\\Fragaria_×_ananassa → processed_data\\train\\Fragaria_x_ananassa\n",
      "Renamed: processed_data\\train\\Mentha_×_piperita → processed_data\\train\\Mentha_x_piperita\n",
      "Renamed: processed_data\\val\\Fragaria_×_ananassa → processed_data\\val\\Fragaria_x_ananassa\n",
      "Renamed: processed_data\\val\\Mentha_×_piperita → processed_data\\val\\Mentha_x_piperita\n",
      "\n",
      "Folder renaming complete.\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = r\"processed_data\"  # your root dataset folder\n",
    "REPLACEMENTS = {\n",
    "    \"×\": \"x\",  # replace multiplication sign with lowercase x\n",
    "}\n",
    "\n",
    "def safe_name(name):\n",
    "    new_name = name\n",
    "    for bad_char, replacement in REPLACEMENTS.items():\n",
    "        new_name = new_name.replace(bad_char, replacement)\n",
    "    return new_name\n",
    "\n",
    "def rename_folders(root_dir):\n",
    "    for current_dir, dirs, files in os.walk(root_dir, topdown=False):\n",
    "        for d in dirs:\n",
    "            new_d = safe_name(d)\n",
    "            if new_d != d:\n",
    "                old_path = os.path.join(current_dir, d)\n",
    "                new_path = os.path.join(current_dir, new_d)\n",
    "                os.rename(old_path, new_path)\n",
    "                print(f\"Renamed: {old_path} → {new_path}\")\n",
    "\n",
    "rename_folders(DATASET_DIR)\n",
    "print(\"\\nFolder renaming complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83002373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "data_folder = \"data\"\n",
    "output_folder = \"images/representatives\"\n",
    "\n",
    "# Make sure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Loop over each subfolder\n",
    "for subfolder in os.listdir(data_folder):\n",
    "    subfolder_path = os.path.join(data_folder, subfolder)\n",
    "    \n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Get all image files in subfolder\n",
    "        images = [f for f in os.listdir(subfolder_path) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "        \n",
    "        if images:\n",
    "            # Pick one random image\n",
    "            random_image = random.choice(images)\n",
    "            random_image_path = os.path.join(subfolder_path, random_image)\n",
    "            \n",
    "            # Clean folder name (replace _ with space)\n",
    "            # clean_name = subfolder.replace(\"_\", \" \")\n",
    "            \n",
    "            # Keep original extension\n",
    "            ext = os.path.splitext(random_image)[1]\n",
    "            \n",
    "            # Destination path\n",
    "            dest_path = os.path.join(output_folder, f\"{subfolder}{ext}\")\n",
    "            \n",
    "            # Copy image\n",
    "            shutil.copy(random_image_path, dest_path)\n",
    "            \n",
    "            print(f\"Picked {random_image} → {dest_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8386b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_wikipedia_image(query, output_path):\n",
    "    url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{query.replace(' ', '_')}\"\n",
    "    response = requests.get(url).json()\n",
    "    \n",
    "    if \"thumbnail\" in response:\n",
    "        img_url = response[\"thumbnail\"][\"source\"]\n",
    "        img_data = requests.get(img_url).content\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            f.write(img_data)\n",
    "        print(f\"Downloaded {query} → {output_path}\")\n",
    "    else:\n",
    "        print(f\"No image found for {query}\")\n",
    "\n",
    "data_folder = \"data\"\n",
    "plants = [subfolder.replace(\"_\", \" \") for subfolder in os.listdir(data_folder)]\n",
    "os.makedirs(\"images/representatives_clean\", exist_ok=True)\n",
    "\n",
    "for plant in plants:\n",
    "    output_path = f\"images/representatives_clean/{plant.replace(' ', '_')}.jpg\"\n",
    "    fetch_wikipedia_image(plant, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e426335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed request for Acacia spp (status 404)\n",
      "Failed request for Acacia_spp (status 404)\n",
      "Failed request for Acer spp (status 404)\n",
      "Failed request for Acer_spp (status 404)\n",
      "Downloaded Achillea millefolium → images/representatives_clean/Achillea_millefolium.jpg\n",
      "Downloaded Actinidia deliciosa → images/representatives_clean/Actinidia_deliciosa.jpg\n",
      "Downloaded Adansonia digitata → images/representatives_clean/Adansonia_digitata.jpg\n",
      "Failed request for Aloe barbadensis miller (status 404)\n",
      "Failed request for Aloe_barbadensis_miller (status 404)\n",
      "Downloaded Anacardium occidentale → images/representatives_clean/Anacardium_occidentale.jpg\n",
      "Downloaded Ananas comosus → images/representatives_clean/Ananas_comosus.jpg\n",
      "Downloaded Antirrhinum majus → images/representatives_clean/Antirrhinum_majus.jpg\n",
      "Failed request for Arecaceae spp (status 404)\n",
      "Failed request for Arecaceae_spp (status 404)\n",
      "Downloaded Artemisia absinthium → images/representatives_clean/Artemisia_absinthium.jpg\n",
      "Downloaded Artocarpus altilis → images/representatives_clean/Artocarpus_altilis.jpg\n",
      "Downloaded Artocarpus heterophyllus → images/representatives_clean/Artocarpus_heterophyllus.jpg\n",
      "Downloaded Azadirachta indica → images/representatives_clean/Azadirachta_indica.jpg\n",
      "Downloaded Bellis perennis → images/representatives_clean/Bellis_perennis.jpg\n",
      "Downloaded Bougainvillea glabra → images/representatives_clean/Bougainvillea_glabra.jpg\n",
      "Downloaded Calendula officinalis → images/representatives_clean/Calendula_officinalis.jpg\n",
      "Downloaded Camellia japonica → images/representatives_clean/Camellia_japonica.jpg\n",
      "Downloaded Carica papaya → images/representatives_clean/Carica_papaya.jpg\n",
      "Downloaded Cassia fistula → images/representatives_clean/Cassia_fistula.jpg\n",
      "Downloaded Ceiba pentandra → images/representatives_clean/Ceiba_pentandra.jpg\n",
      "Downloaded Chrysanthemum indicum → images/representatives_clean/Chrysanthemum_indicum.jpg\n",
      "Downloaded Citrullus lanatus → images/representatives_clean/Citrullus_lanatus.jpg\n",
      "Downloaded Citrus limon → images/representatives_clean/Citrus_limon.jpg\n",
      "Downloaded Citrus sinensis → images/representatives_clean/Citrus_sinensis.jpg\n",
      "Downloaded Cocos nucifera → images/representatives_clean/Cocos_nucifera.jpg\n",
      "Downloaded Coriandrum sativum → images/representatives_clean/Coriandrum_sativum.jpg\n",
      "Downloaded Cosmos bipinnatus → images/representatives_clean/Cosmos_bipinnatus.jpg\n",
      "Downloaded Curcuma longa → images/representatives_clean/Curcuma_longa.jpg\n",
      "Downloaded Cymbopogon citratus → images/representatives_clean/Cymbopogon_citratus.jpg\n",
      "Downloaded Dahlia pinnata → images/representatives_clean/Dahlia_pinnata.jpg\n",
      "Downloaded Delonix regia → images/representatives_clean/Delonix_regia.jpg\n",
      "Downloaded Dianthus caryophyllus → images/representatives_clean/Dianthus_caryophyllus.jpg\n",
      "Downloaded Echinacea purpurea → images/representatives_clean/Echinacea_purpurea.jpg\n",
      "Downloaded Eucalyptus globulus → images/representatives_clean/Eucalyptus_globulus.jpg\n",
      "Downloaded Ficus benghalensis → images/representatives_clean/Ficus_benghalensis.jpg\n",
      "Downloaded Ficus carica → images/representatives_clean/Ficus_carica.jpg\n",
      "Downloaded Fragaria x ananassa → images/representatives_clean/Fragaria_x_ananassa.jpg\n",
      "Failed request for Gladiolus spp (status 404)\n",
      "Failed request for Gladiolus_spp (status 404)\n",
      "Downloaded Helianthus annuus → images/representatives_clean/Helianthus_annuus.jpg\n",
      "Downloaded Hevea brasiliensis → images/representatives_clean/Hevea_brasiliensis.jpg\n",
      "Downloaded Hibiscus rosa-sinensis → images/representatives_clean/Hibiscus_rosa-sinensis.jpg\n",
      "Downloaded Hylocereus undatus → images/representatives_clean/Hylocereus_undatus.jpg\n",
      "Downloaded Ipomoea purpurea → images/representatives_clean/Ipomoea_purpurea.jpg\n",
      "Downloaded Jacaranda mimosifolia → images/representatives_clean/Jacaranda_mimosifolia.jpg\n",
      "Downloaded Jasminum sambac → images/representatives_clean/Jasminum_sambac.jpg\n",
      "Downloaded Lantana camara → images/representatives_clean/Lantana_camara.jpg\n",
      "Downloaded Lavandula angustifolia → images/representatives_clean/Lavandula_angustifolia.jpg\n",
      "Failed request for Lilium spp (status 404)\n",
      "Failed request for Lilium_spp (status 404)\n",
      "Downloaded Litchi chinensis → images/representatives_clean/Litchi_chinensis.jpg\n",
      "Downloaded Malus domestica → images/representatives_clean/Malus_domestica.jpg\n",
      "Downloaded Mangifera indica → images/representatives_clean/Mangifera_indica.jpg\n",
      "Downloaded Matricaria chamomilla → images/representatives_clean/Matricaria_chamomilla.jpg\n",
      "Failed request for Mentha spp (status 404)\n",
      "Failed request for Mentha_spp (status 404)\n",
      "Downloaded Mentha x piperita → images/representatives_clean/Mentha_x_piperita.jpg\n",
      "Downloaded Moringa oleifera → images/representatives_clean/Moringa_oleifera.jpg\n",
      "Downloaded Morus alba → images/representatives_clean/Morus_alba.jpg\n",
      "Downloaded Musa spp → images/representatives_clean/Musa_spp.jpg\n",
      "Failed request for Narcissus spp (status 404)\n",
      "Failed request for Narcissus_spp (status 404)\n",
      "Downloaded Nelumbo nucifera → images/representatives_clean/Nelumbo_nucifera.jpg\n",
      "Downloaded Ocimum basilicum → images/representatives_clean/Ocimum_basilicum.jpg\n",
      "Downloaded Ocimum tenuiflorum → images/representatives_clean/Ocimum_tenuiflorum.jpg\n",
      "Downloaded Olea europaea → images/representatives_clean/Olea_europaea.jpg\n",
      "Failed request for Orchidaceae spp (status 404)\n",
      "Failed request for Orchidaceae_spp (status 404)\n",
      "Downloaded Origanum vulgare → images/representatives_clean/Origanum_vulgare.jpg\n",
      "Downloaded Papaver rhoeas → images/representatives_clean/Papaver_rhoeas.jpg\n",
      "Downloaded Passiflora edulis → images/representatives_clean/Passiflora_edulis.jpg\n",
      "Failed request for Pelargonium spp (status 404)\n",
      "Failed request for Pelargonium_spp (status 404)\n",
      "Downloaded Persea americana → images/representatives_clean/Persea_americana.jpg\n",
      "Failed request for Petunia spp (status 404)\n",
      "Failed request for Petunia_spp (status 404)\n",
      "Downloaded Phoenix dactylifera → images/representatives_clean/Phoenix_dactylifera.jpg\n",
      "Failed request for Pinus spp (status 404)\n",
      "Failed request for Pinus_spp (status 404)\n",
      "Downloaded Plantago major → images/representatives_clean/Plantago_major.jpg\n",
      "Downloaded Prunus avium → images/representatives_clean/Prunus_avium.jpg\n",
      "Downloaded Prunus domestica → images/representatives_clean/Prunus_domestica.jpg\n",
      "Downloaded Psidium guajava → images/representatives_clean/Psidium_guajava.jpg\n",
      "Downloaded Punica granatum → images/representatives_clean/Punica_granatum.jpg\n",
      "Downloaded Pyrus communis → images/representatives_clean/Pyrus_communis.jpg\n",
      "Failed request for Quercus spp (status 404)\n",
      "Failed request for Quercus_spp (status 404)\n",
      "Downloaded Rosa indica → images/representatives_clean/Rosa_indica.jpg\n",
      "Downloaded Salvia officinalis → images/representatives_clean/Salvia_officinalis.jpg\n",
      "Downloaded Salvia rosmarinus → images/representatives_clean/Salvia_rosmarinus.jpg\n",
      "Downloaded Solanum betaceum → images/representatives_clean/Solanum_betaceum.jpg\n",
      "Downloaded Swietenia macrophylla → images/representatives_clean/Swietenia_macrophylla.jpg\n",
      "Downloaded Tagetes erecta → images/representatives_clean/Tagetes_erecta.jpg\n",
      "Downloaded Tamarindus indica → images/representatives_clean/Tamarindus_indica.jpg\n",
      "Downloaded Taraxacum officinale → images/representatives_clean/Taraxacum_officinale.jpg\n",
      "Downloaded Tectona grandis → images/representatives_clean/Tectona_grandis.jpg\n",
      "Downloaded Theobroma cacao → images/representatives_clean/Theobroma_cacao.jpg\n",
      "Downloaded Thymus vulgaris → images/representatives_clean/Thymus_vulgaris.jpg\n",
      "Failed request for Trifolium spp (status 404)\n",
      "Failed request for Trifolium_spp (status 404)\n",
      "Failed request for Tulipa spp (status 404)\n",
      "Failed request for Tulipa_spp (status 404)\n",
      "Downloaded Urtica dioica → images/representatives_clean/Urtica_dioica.jpg\n",
      "Downloaded Vitis vinifera → images/representatives_clean/Vitis_vinifera.jpg\n",
      "Downloaded Zingiber officinale → images/representatives_clean/Zingiber_officinale.jpg\n",
      "Downloaded Zinnia elegans → images/representatives_clean/Zinnia_elegans.jpg\n"
     ]
    }
   ],
   "source": [
    "HEADERS = {\n",
    "    \"User-Agent\": \"PlantClassifierBot/1.0 (https://github.com/themrandroid; contact: rasheedmuhammed002@gmail.com)\"\n",
    "}\n",
    "\n",
    "def fetch_wikipedia_image(query, output_path):\n",
    "    url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{query.replace(' ', '_')}\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed request for {query} (status {response.status_code})\")\n",
    "            return False\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        if \"thumbnail\" in data:\n",
    "            img_url = data[\"thumbnail\"][\"source\"]\n",
    "            img_data = requests.get(img_url, headers=HEADERS).content\n",
    "            with open(output_path, \"wb\") as f:\n",
    "                f.write(img_data)\n",
    "            print(f\"Downloaded {query} → {output_path}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"No thumbnail for {query}\")\n",
    "            return False\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request error for {query}: {e}\")\n",
    "        return False\n",
    "    except ValueError:\n",
    "        print(f\"JSON parse error for {query}\")\n",
    "        return False\n",
    "\n",
    "# Main loop\n",
    "data_folder = \"data\"\n",
    "plants = [subfolder.replace(\"_\", \" \") for subfolder in os.listdir(data_folder)]\n",
    "os.makedirs(\"images/representatives_clean\", exist_ok=True)\n",
    "\n",
    "for plant in plants:\n",
    "    output_path = f\"images/representatives_clean/{plant.replace(' ', '_')}.jpg\"\n",
    "    \n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Skipping {plant}, already exists.\")\n",
    "        continue\n",
    "    \n",
    "    if not fetch_wikipedia_image(plant, output_path):\n",
    "        fetch_wikipedia_image(plant.replace(\" \", \"_\"), output_path)\n",
    "    \n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
